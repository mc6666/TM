{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Categorization with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reference: [How we Changed Unsupervised LDA to Semi-Supervised GuidedLDA](https://www.freecodecamp.org/news/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164/)\n",
    "### code：[GuidedLDA: Guided Topic modeling with latent Dirichlet allocation](https://github.com/vi3k6i5/GuidedLDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE='./WF Cases 20190517.csv'\n",
    "TRAINING_FIELD=['Subject', 'Description']\n",
    "EPOCHS=100\n",
    "\n",
    "seed_topic_list = [['configuration', 'network drive','cisco', 'vpn','firewall','active directory'],\n",
    "                   ['false_positive', 'url reclassification', 'whitelist', 'white listing','block','unauthorized encryption'],\n",
    "                   ['sa offline', 'corruption','behavior monitor','web console'],\n",
    "                   ['compatibility','3rd party','update', 'mcafee', 'msa', 'exchange server','quickbook','office'],\n",
    "                   ['performance','high_cpu', 'crash','hang','slowness','office','bsod','3rd party'],\n",
    "                   ['license', 'activation','clp','expire', 'renew', 'activate','invalid','merge','account', 'password'],\n",
    "                   ['document', 'kb', 'bpg', 'article','password'],\n",
    "                   ['update', 'agent','hotfix','patch','migrate'],\n",
    "                   ['deployment', 'removal', 'server', 'agent','install','uninstall','reinstall','cut'],\n",
    "                   ['threat','virus', 'scan', 'malware', 'detect', 'infection','ransom']\n",
    "                  ]\n",
    "\n",
    "# 常出現的單字而且是名詞就排除，兩種方法：\n",
    "#   1. 設定特定單字，如果頻率大於該單字則排除，此法優先(word_to_exclude_from_vocabulary)\n",
    "#   2. 設定頻率，大於該頻率的單字則排除\n",
    "# frequency = no. of documents / total documents\n",
    "frequency_to_exclude_from_vocabulary = 0.01\n",
    "word_to_exclude_from_vocabulary = 'agent'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert seed_topic_list and calculate no. of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_keyword_list = []\n",
    "for i in range(len(seed_topic_list)):\n",
    "    for j in range(len(seed_topic_list[i])):\n",
    "        seed_topic_list[i][j]=seed_topic_list[i][j].strip()\n",
    "        if ' ' in seed_topic_list[i][j]:\n",
    "            extra_keyword_list.append(seed_topic_list[i][j])\n",
    "            seed_topic_list[i][j]=seed_topic_list[i][j].replace(' ', '_')\n",
    "\n",
    "extra_keyword_list=list(set(extra_keyword_list))            \n",
    "n_topics=len(seed_topic_list)\n",
    "n_topics, len(extra_keyword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load List of Microsoft software and Ubuntu Glossaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import scrape_MS_Linux_keywords\n",
    "\n",
    "#if not os.path.exists(\"keyword_list.pickle\"):\n",
    "scrape_MS_Linux_keywords.scrap_keywords()\n",
    "    \n",
    "with open(\"keyword_list.pickle\", 'rb') as f:\n",
    "    keyword_list_TERM = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyword_list_TERM.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add extra keywords into keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extra keywords into keyword_list\n",
    "for item in extra_keyword_list:\n",
    "    if not item in keyword_list_TERM:\n",
    "        keyword_list_TERM[item]={'converted_word': item.replace(' ','_'), 'count': 1}        \n",
    "\n",
    "# save back\n",
    "with open(\"keyword_list.pickle\", 'wb') as f:\n",
    "    pickle.dump(keyword_list_TERM, f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Description</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Version</th>\n",
       "      <th>Date/Time Opened</th>\n",
       "      <th>Solution Provided</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00677507</td>\n",
       "      <td>[WFBS-SVC] Installation on Mac Is Not Full Fea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Worry-Free Business Security Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/2018 12:18 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00677513</td>\n",
       "      <td>[WFBS - A] Smart Scan is not updating</td>\n",
       "      <td>CRC proc crash instability unable to keep syst...</td>\n",
       "      <td>Worry-Free Business Security Advanced</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1/1/2018 2:30 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00677518</td>\n",
       "      <td>[WFBS - SVC] Installation issue</td>\n",
       "      <td>Migrated a client to a new computer, the softw...</td>\n",
       "      <td>Worry-Free Business Security Services</td>\n",
       "      <td>6.2 (Beta)</td>\n",
       "      <td>1/1/2018 6:41 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00677669</td>\n",
       "      <td>[MALWARE][WFBS SVC] Wallet Ransomware</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Worry-Free Business Security Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/2018 11:54 PM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00677682</td>\n",
       "      <td>[MALWARE][WFBS S 9.5]Possible Ransomware detec...</td>\n",
       "      <td>OMAN wfbs SYED AIJAZ LOAY INTERNATIONAL LLC ai...</td>\n",
       "      <td>Worry-Free Business Security Standard</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1/2/2018 12:30 AM</td>\n",
       "      <td>Next Action Plan: - check detection on the con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00677799</td>\n",
       "      <td>[WFBS-S] Adding Exclusion to Behavior Monitoring</td>\n",
       "      <td>[Case Description] - client SWA financial plan...</td>\n",
       "      <td>Worry-Free Business Security Standard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/2/2018 10:37 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00677803</td>\n",
       "      <td>[WFBS-S] Migrating from An Old Server to A New...</td>\n",
       "      <td>Issue: - customer wants to migrate WFBS to new...</td>\n",
       "      <td>Worry-Free Business Security Standard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/2/2018 10:49 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00677816</td>\n",
       "      <td>[WFBS-SVC] Agent cloning</td>\n",
       "      <td>Can Worry Free agent with OS be image cloned/g...</td>\n",
       "      <td>Worry-Free Business Security Services</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1/2/2018 12:02 PM</td>\n",
       "      <td>- manually reset the GUID info on the registry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00677831</td>\n",
       "      <td>[WFBS-SVC] Main Software not Functioning with ...</td>\n",
       "      <td>The latest Trend Micro update prevents one of ...</td>\n",
       "      <td>Worry-Free Business Security Services</td>\n",
       "      <td>6.2 (Beta)</td>\n",
       "      <td>1/2/2018 1:13 PM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00677857</td>\n",
       "      <td>[WFBS-SVC] False Positive: Application Detecte...</td>\n",
       "      <td>[Issue]: False Positive: Application Detected ...</td>\n",
       "      <td>Worry-Free Business Security Standard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/2/2018 2:38 PM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Case Number                                            Subject  \\\n",
       "0    00677507  [WFBS-SVC] Installation on Mac Is Not Full Fea...   \n",
       "1    00677513              [WFBS - A] Smart Scan is not updating   \n",
       "2    00677518                    [WFBS - SVC] Installation issue   \n",
       "3    00677669              [MALWARE][WFBS SVC] Wallet Ransomware   \n",
       "4    00677682  [MALWARE][WFBS S 9.5]Possible Ransomware detec...   \n",
       "5    00677799   [WFBS-S] Adding Exclusion to Behavior Monitoring   \n",
       "6    00677803  [WFBS-S] Migrating from An Old Server to A New...   \n",
       "7    00677816                           [WFBS-SVC] Agent cloning   \n",
       "8    00677831  [WFBS-SVC] Main Software not Functioning with ...   \n",
       "9    00677857  [WFBS-SVC] False Positive: Application Detecte...   \n",
       "\n",
       "                                         Description  \\\n",
       "0                                                NaN   \n",
       "1  CRC proc crash instability unable to keep syst...   \n",
       "2  Migrated a client to a new computer, the softw...   \n",
       "3                                                NaN   \n",
       "4  OMAN wfbs SYED AIJAZ LOAY INTERNATIONAL LLC ai...   \n",
       "5  [Case Description] - client SWA financial plan...   \n",
       "6  Issue: - customer wants to migrate WFBS to new...   \n",
       "7  Can Worry Free agent with OS be image cloned/g...   \n",
       "8  The latest Trend Micro update prevents one of ...   \n",
       "9  [Issue]: False Positive: Application Detected ...   \n",
       "\n",
       "                            Product Name Product Version   Date/Time Opened  \\\n",
       "0  Worry-Free Business Security Services             NaN  1/1/2018 12:18 AM   \n",
       "1  Worry-Free Business Security Advanced             9.5   1/1/2018 2:30 AM   \n",
       "2  Worry-Free Business Security Services      6.2 (Beta)   1/1/2018 6:41 AM   \n",
       "3  Worry-Free Business Security Services             NaN  1/1/2018 11:54 PM   \n",
       "4  Worry-Free Business Security Standard             9.5  1/2/2018 12:30 AM   \n",
       "5  Worry-Free Business Security Standard             NaN  1/2/2018 10:37 AM   \n",
       "6  Worry-Free Business Security Standard             NaN  1/2/2018 10:49 AM   \n",
       "7  Worry-Free Business Security Services             6.2  1/2/2018 12:02 PM   \n",
       "8  Worry-Free Business Security Services      6.2 (Beta)   1/2/2018 1:13 PM   \n",
       "9  Worry-Free Business Security Standard             NaN   1/2/2018 2:38 PM   \n",
       "\n",
       "                                   Solution Provided  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4  Next Action Plan: - check detection on the con...  \n",
       "5                                                NaN  \n",
       "6                                                NaN  \n",
       "7     - manually reset the GUID info on the registry  \n",
       "8                                                NaN  \n",
       "9                                                NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(TRAINING_FILE)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Description</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Version</th>\n",
       "      <th>Date/Time Opened</th>\n",
       "      <th>Solution Provided</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10626</th>\n",
       "      <td>00874657</td>\n",
       "      <td>URL Reclassification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Worry-Free Business Security Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/25/2018 12:52 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29475</th>\n",
       "      <td>01732305</td>\n",
       "      <td>url reclassification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Worry-Free Business Security Services</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2/21/2019 1:53 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Case Number               Subject Description  \\\n",
       "10626    00874657  URL Reclassification         NaN   \n",
       "29475    01732305  url reclassification         NaN   \n",
       "\n",
       "                                Product Name Product Version  \\\n",
       "10626  Worry-Free Business Security Services             NaN   \n",
       "29475  Worry-Free Business Security Services             5.7   \n",
       "\n",
       "         Date/Time Opened Solution Provided  \n",
       "10626  5/25/2018 12:52 AM               NaN  \n",
       "29475   2/21/2019 1:53 AM               NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Subject.str.lower().isin(extra_keyword_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import krovetz\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "break_words=list('\\n\\t')\n",
    "prefix_char_remove=list('.*,-')\n",
    "\n",
    "stemmer = krovetz.PyKrovetzStemmer()\n",
    "#stemmer = SnowballStemmer(\"english\")\n",
    "#stemmer.stem('working')\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "word_freqs = dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定 regular expression ，以去除 email、數字、URL、Phone No、序號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_pattern_list=[]\n",
    "re_pattern_list.append('[a-zA-Z0-9_]+\\.(com|org|net)[a-zA-Z0-9_\\.]*$') #email\n",
    "re_pattern_list.append('[0-9_]+') # all digits\n",
    "re_pattern_list.append(\"\\/\\/[\\w.-]+(?:\\.[\\w\\.-]+)+[\\w\\-\\._~:/?#[\\]@!\\$&'\\(\\)\\*\\+,;=.]+$\") # url\n",
    "re_pattern_list.append('[0-9\\-]+') # phone no\n",
    "re_pattern_list.append('[\\u0800-\\u4e00]+') # 日文\n",
    "re_pattern_list.append('(trend|trend\\smicro)') # trend micro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字詞清理函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean words\n",
    "def clean_word(word_0):\n",
    "    word_list=[]\n",
    "    for word in word_0.split('/'):\n",
    "\n",
    "        if len(word) <= 0 :\n",
    "            continue\n",
    "\n",
    "        # remove 網路芳鄰路徑\n",
    "        remote_path=word.split('\\\\')\n",
    "        if len(remote_path) > 0 and len(remote_path[-1])>0:\n",
    "            word=remote_path[-1]\n",
    "        elif len(remote_path) > 1 and len(remote_path[-2])>0:\n",
    "            word=remote_path[-2]\n",
    "        elif len(remote_path) > 2 and len(remote_path[-3])>0:\n",
    "            word=remote_path[-3]\n",
    "\n",
    "        if word[0] == '\"' and word[-1] == '\"':\n",
    "            word=word[1:-1]\n",
    "        if word[0] == \"'\" and word[-1] == \"'\":\n",
    "            word=word[1:-1]\n",
    "        if len(word) <= 0 :\n",
    "            continue\n",
    "\n",
    "        # repeat 3 times to remove continous chars\n",
    "        if word[0] in prefix_char_remove:\n",
    "            word=word[1:]\n",
    "        if len(word) <= 0 :\n",
    "            continue\n",
    "        if word[0] in prefix_char_remove:\n",
    "            word=word[1:]\n",
    "        if len(word) <= 0 :\n",
    "            continue\n",
    "        if word[0] in prefix_char_remove:\n",
    "            word=word[1:]\n",
    "\n",
    "        # lemmatize\n",
    "        word=word.strip().lower()\n",
    "        word=stemmer.stem(word)\n",
    "        if word=='':\n",
    "            continue      \n",
    "            \n",
    "        # remove .exe and .com\n",
    "        if word.endswith('.exe') or word.endswith('.com'):\n",
    "            word=word[:-4]\n",
    "            \n",
    "        # 以 regular expression 去除 email、數字、URL、Phone No、序號\n",
    "        is_re_list = False\n",
    "        for pattern1 in re_pattern_list:\n",
    "            result=re.findall(pattern1, word)\n",
    "            if  len(result) > 0:\n",
    "                is_re_list = True\n",
    "                break\n",
    "        if is_re_list:\n",
    "            break\n",
    "            \n",
    "            \n",
    "        # remove the words with one or two characters only\n",
    "        if len(word) <= 2 :\n",
    "            continue\n",
    "        if not (word in stop_words):\n",
    "            word_list.append(word)\n",
    "               \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test get service_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WFBS-SVC] Installation on Mac Is Not Full Featured\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "for item in TRAINING_FIELD:\n",
    "    print(df[item][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[WFBS-SVCaaa]', '[WFBS-SVC]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern1 = '\\[[a-zA-Z0-9_\\-]+\\]' # category\n",
    "re.findall(pattern1, '[WFBS-SVCaaa]'+df.Subject[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36124\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern1 = '\\[[a-zA-Z0-9_\\-\\s]+\\]' # service_list pattern, e.g. [WFBS-SVC]\n",
    "\n",
    "# 清理後的 email DESCRIPTION\n",
    "clean_corpus=[]\n",
    "original_corpus=[]\n",
    "for index, line in df.iterrows():\n",
    "    clean_line=\"\"\n",
    "    line2=''\n",
    "    for item in TRAINING_FIELD:\n",
    "        #print(item)\n",
    "        if type(line[item]) == str and len(line[item])>0:\n",
    "            line2 += ' ' + line[item]\n",
    "    line=line2.strip()\n",
    "    \n",
    "    # remove service_list, e.g. [WFBS-SVC]\n",
    "    if type(line) == str and len(line) > 0: \n",
    "        service_list = re.findall(pattern1, line)\n",
    "        for service_item in service_list:\n",
    "            line = line.replace(service_item, ' ')          \n",
    "                                  \n",
    "    #print(line)\n",
    "    for break_word in break_words:\n",
    "        #print('-',len(break_word), break_word, '-')\n",
    "        if not type(line) == str or len(line) <= 0: \n",
    "            break\n",
    "        line = line.replace(break_word, ' ')\n",
    "    if not type(line) == str or len(line) <= 0: \n",
    "        #print(type(line))\n",
    "        continue\n",
    "    words = word_tokenize(line) #line.lower().split(' ')\n",
    "    for word_0 in words:        \n",
    "        word_list = clean_word(word_0)\n",
    "        for word in word_list:        \n",
    "            if word in word_freqs:\n",
    "                word_freqs[word] += 1\n",
    "            else:\n",
    "                word_freqs[word] = 1\n",
    "            clean_line+=' '+word\n",
    "    original_corpus.append(line)\n",
    "    clean_corpus.append(clean_line.strip())\n",
    "email_words=word_freqs.keys()            \n",
    "print(len(email_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以 regular expression 去除 email、數字、URL、Phone No、序號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36124"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "keyword_list_new=[]\n",
    "keyword_list = set(email_words)\n",
    "for item in keyword_list:\n",
    "    is_re_list = False\n",
    "    for pattern1 in re_pattern_list:\n",
    "        result=re.findall(pattern1, item)\n",
    "        if  len(result) > 0:\n",
    "            is_re_list = True\n",
    "            break\n",
    "    if is_re_list == False:\n",
    "        keyword_list_new.append(item)\n",
    "len(keyword_list_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare corpus with n-gram keywords, n=2~4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if key word is found \n",
    "def check_special_term(words, start_index, n_gram):\n",
    "    merge_word=[]\n",
    "    for i in range(n_gram):\n",
    "        merge_word.append(words[start_index+i])\n",
    "    if ' '.join(merge_word) in keyword_list_TERM:\n",
    "        #print('converted_word=', keyword_list_TERM[' '.join(merge_word)]['converted_word'])\n",
    "        return keyword_list_TERM[' '.join(merge_word)]['converted_word']\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# scan corpus and replace key word by converted_word, which use underscope instead of space     \n",
    "def compare_corpus_with_keyword(clean_corpus):\n",
    "    new_clean_corpus=[]\n",
    "    merge_word_list=[] # n-gram keywords\n",
    "    hit_row_index=[] # keep for predict test\n",
    "    for no, line in enumerate(clean_corpus):\n",
    "        words = line.split(' ')\n",
    "        #words = word_tokenize(line) \n",
    "        words_len = len(words)\n",
    "        # n-gram\n",
    "        new_line=''\n",
    "        for i in range(4, 1, -1):\n",
    "            for j in range(words_len-i+1):\n",
    "                merge_word = check_special_term(words, j, i)\n",
    "                if merge_word == '':\n",
    "                    new_line+=' '+words[j]\n",
    "                    if j==words_len-i:\n",
    "                        for k in range(j+1, words_len):\n",
    "                            new_line+=' '+words[k]\n",
    "                    continue\n",
    "                else:\n",
    "                    merge_word_list.append(merge_word)\n",
    "                    hit_row_index.append(no)\n",
    "                    new_line+=' '+merge_word\n",
    "                    j+=i\n",
    "                    continue\n",
    "\n",
    "            # line is too short, keep it as original\n",
    "            if words_len-i<0:\n",
    "                new_line=line    \n",
    "            new_line = new_line.strip()\n",
    "            words = new_line.split(' ')\n",
    "            words_len = len(words)\n",
    "            if i>2 and words != 'kb':\n",
    "                new_line=''\n",
    "        new_clean_corpus.append(new_line)\n",
    "        #if new_line != line:\n",
    "        #    print('org_line='+line+'\\n')\n",
    "        #    print('new_line='+new_line+'\\n\\n')\n",
    "    return new_clean_corpus, merge_word_list, hit_row_index\n",
    "\n",
    "new_clean_corpus, merge_word_list, hit_row_index = compare_corpus_with_keyword(clean_corpus)\n",
    "hit_row_index=list(set(hit_row_index))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(new_clean_corpus)\n",
    "X = tfidf.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "#vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常出現的單字而且是名詞就排除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count= 9749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34229"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "exclude_word_list=[]\n",
    "pos_tag = nltk.pos_tag(vocab)\n",
    "\n",
    "if word_to_exclude_from_vocabulary != None and word_to_exclude_from_vocabulary != '' and word_to_exclude_from_vocabulary in vectorizer.vocabulary_.keys():\n",
    "    word_index_to_exclude_from_vocabulary = vectorizer.vocabulary_[word_to_exclude_from_vocabulary]\n",
    "    min_count = np.sum(X[:, word_index_to_exclude_from_vocabulary] > 0)\n",
    "    print('min_count=', min_count)\n",
    "    for i, word in enumerate(vocab):\n",
    "        if np.sum(X[:, i] > 0) >= min_count and pos_tag[i][1].startswith('NN'):\n",
    "            exclude_word_list.append(i)\n",
    "elif frequency_to_exclude_from_vocabulary != None and frequency_to_exclude_from_vocabulary > 0:\n",
    "    min_count = frequency_to_exclude_from_vocabulary * len(vocab)\n",
    "    for i, word in enumerate(vocab):\n",
    "        if np.sum(X[:, i] > 0) >= min_count and pos_tag[i][1].startswith('NN'):\n",
    "            exclude_word_list.append(i)\n",
    "                \n",
    "for i in reversed(exclude_word_list):\n",
    "    del vocab[i]\n",
    "    \n",
    "# regenerate vocabulary\n",
    "vocabulary_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocabulary_dict[word]=i\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34230"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW=(34016, 34229), no. of vocabulary=34229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary_dict)\n",
    "BOW_vector = vectorizer.fit_transform(new_clean_corpus)\n",
    "X = BOW_vector.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "pickle.dump(vectorizer.vocabulary_,open(\"feature.pkl\",\"wb\"))\n",
    "print('BOW={}, no. of vocabulary={}'.format(X.shape, len(vocab)))\n",
    "\n",
    "word2id = dict((v, idx) for idx, v in enumerate(vocab))\n",
    "pickle.dump(word2id,open(\"word2id.pkl\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary from vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import guidedlda\n",
    "\n",
    "#X = guidedlda.datasets.load_data(guidedlda.datasets.NYT)\n",
    "#vocab = guidedlda.datasets.load_vocab(guidedlda.datasets.NYT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加重特定單字在某些Topics的權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 34016\n",
      "INFO:guidedlda:vocab_size: 34229\n",
      "INFO:guidedlda:n_words: 752620\n",
      "INFO:guidedlda:n_topics: 10\n",
      "INFO:guidedlda:n_iter: 100\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -8248320\n",
      "INFO:guidedlda:<20> log likelihood: -5530164\n",
      "INFO:guidedlda:<40> log likelihood: -5400652\n",
      "INFO:guidedlda:<60> log likelihood: -5351443\n",
      "INFO:guidedlda:<80> log likelihood: -5324041\n",
      "INFO:guidedlda:<99> log likelihood: -5305588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x2238cfe3d68>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guided LDA with seed topics.\n",
    "\n",
    "model = guidedlda.GuidedLDA(n_topics=n_topics, n_iter=EPOCHS, refresh=20)\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        if not word in word2id.keys():\n",
    "            word2id[word] = t_id\n",
    "\n",
    "        seed_topics[word2id[word]] = t_id\n",
    "model.fit(X, seed_topics=seed_topics, seed_confidence=0.15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示每個Topics前八個重要的單字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: server security install client console micro wfb business device use\n",
      "Keywords: ['configuration', 'network_drive', 'cisco', 'vpn', 'firewall', 'active_directory']\n",
      "Topic 1: block url http application file website filter add program list\n",
      "Keywords: ['false_positive', 'url_reclassification', 'whitelist', 'white_listing', 'block', 'unauthorized_encryption']\n",
      "Topic 2: name email company number contact address customer product phone version\n",
      "Keywords: ['sa_offline', 'corruption', 'behavior_monitor', 'web_console']\n",
      "Topic 3: channel window problem security business name description detail session status\n",
      "Keywords: ['compatibility', '3rd_party', 'update', 'mcafee', 'msa', 'exchange_server', 'quickbook', 'office']\n",
      "Topic 4: name company email customer product number contact address version security\n",
      "Keywords: ['performance', 'high_cpu', 'crash', 'hang', 'slowness', 'office', 'bsod', '3rd_party']\n",
      "Topic 5: license account activate seat customer new code renew product key\n",
      "Keywords: ['license', 'activation', 'clp', 'expire', 'renew', 'activate', 'invalid', 'merge', 'account', 'password']\n",
      "Topic 6: console server access issue error service security web_console password unable\n",
      "Keywords: ['document', 'kb', 'bpg', 'article', 'password']\n",
      "Topic 7: update server security patch version window client install issue upgrade\n",
      "Keywords: ['update', 'agent', 'hotfix', 'patch', 'migrate']\n",
      "Topic 8: install uninstall security tool server installation try error client unable\n",
      "Keywords: ['deployment', 'removal', 'server', 'agent', 'install', 'uninstall', 'reinstall', 'cut']\n",
      "Topic 9: file scan detect virus program threat ransomware encryption detection false\n",
      "Keywords: ['threat', 'virus', 'scan', 'malware', 'detect', 'infection', 'ransom']\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "    print('Keywords: {}'.format(seed_topic_list[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 34229)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "'white_listing' in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示每個Document所屬的Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: [WFBS-SVC] Installation on Mac Is Not Full Featured\n",
      "top topic: 0\n",
      "Document: feature, mac, installation, full\n",
      " \n",
      "原文: [WFBS - A] Smart Scan is not updating\n",
      "top topic: 7\n",
      "Document: smtp, scan, smart, crash, unable, system, crc, keep, green, update, failure, proc, instability\n",
      " \n",
      "原文: [WFBS - SVC] Installation issue\n",
      "top topic: 8\n",
      "Document: install, new, computer, micro, unable, software, uninstall, migrate, fail, security, installation, client, part, issue, cleanly\n",
      " \n",
      "原文: [MALWARE][WFBS SVC] Wallet Ransomware\n",
      "top topic: 1\n",
      "Document: wallet, ransomware\n",
      " \n",
      "原文: [MALWARE][WFBS S 9.5]Possible Ransomware detection on clients machine\n",
      "top topic: 0\n",
      "Document: wfb, aijaz, client, oman, machine, loay, loayoman, possible, ransomware, llc, international, detection, syed\n",
      " \n",
      "原文: [WFBS-S] Adding Exclusion to Behavior Monitoring\n",
      "top topic: 1\n",
      "Document: exclusion, set, unc, app, always, path, including, whenever, swa, want, without, change, add, behavior_monitor, financial, file, know, exe, would, plan, update, need, monitor, client\n",
      " \n",
      "原文: [WFBS-S] Migrating from An Old Server to A New Server\n",
      "top topic: 8\n",
      "Document: server, migrate, new, old, well, customer, wfb, wants, issue\n",
      " \n",
      "原文: [WFBS-SVC] Agent cloning\n",
      "top topic: 8\n",
      "Document: new, clone, free, another, remark, ghost, restore, image, laptop, pcs, worry\n",
      " \n",
      "原文: [WFBS-SVC] Main Software not Functioning with Trend Micro Security Agent\n",
      "top topic: 7\n",
      "Document: function, software, micro, main, menu, use, follow, path, latest, try, option, info, portal, security, one, web, online, prevent, available, full, exclusion, update, set, work, however, need\n",
      " \n",
      "原文: [WFBS-SVC] False Positive: Application Detected as A Trojan\n",
      "top topic: 9\n",
      "Document: trojan, false, positive, application, detect\n",
      " \n",
      "原文: [WFBS-A 9.0 SP3] Active Update folder consumes large disk space (84 GB)\n",
      "top topic: 7\n",
      "Document: disk, consume, space, large, update, active, folder\n",
      " \n",
      "原文: [WFBS-A] Getting Outbreak Defense Notifications, Cannot Enter Web Administrator to Check or Disable Message\n",
      "top topic: 9\n",
      "Document: outbreak, administrator, check, message, enter, web, get, disable, following, server, mui, security, file, execute, network, compress, detect, deny, audioses, dll, defense, notification, prevent, spread, risk, access\n",
      " \n",
      "原文: [WFBS-A 9.0] Uninstall SA\n",
      "top topic: 8\n",
      "Document: uninstall\n",
      " \n",
      "原文: [WFBS-SVC] Getting Error Message \"An error has stopped the Trend Micro Security Agent from installing.\"\n",
      "top topic: 8\n",
      "Document: error, install, micro, security, message, stop, running, attach, change, later, try, please, get, thanks, see, encounter, computer, software, made, pls\n",
      " \n",
      "原文: [WFBS-SVC] BM-FA\n",
      "top topic: 9\n",
      "Document: encryption, unauthorized, list, customer, description, tag, add, file, case, already, program, exclusion\n",
      " \n",
      "原文: [WFBS 9.5-A] BM-FA | archivo.exe\n",
      "top topic: 9\n",
      "Document: action, next, log, list, exe, path, check, add, nomina, detect, behavior_monitor, wfb, issue, whitelist, program, plan, archivo, copy, exclusion, taken, request, monitor, detection, full\n",
      " \n",
      "原文: [WFBS-SVC] URL Reclassification\n",
      "top topic: 1\n",
      "Document: bizimply, blog, www, customer, case, block, atss, url, micro, issue, employee, safe, experience, cannon, marked, email, online, advise, detail, schedule, mikey, account, security, website, correct, region, domain, address, name, url_reclassification, one, number, submit, access, reclasification, reclassification, browse, michael, provide, rectify, could, please, solution, situation, blacklist, clean, malware, last, contact, thank, fixed, emea, google, title, week\n",
      " \n",
      "原文: [WFBS SVC] License (Merge) Issue\n",
      "top topic: 5\n",
      "Document: license, merge, account, add, basis, contain, know, issue, get, email, detail, host, security, remove, use, please, error, regards, following, contact, certainly, would, choice, like, daily, try, richard, delete, nellestein, hello, current, support\n",
      " \n",
      "原文: RTL: False Positive\n",
      "top topic: 9\n",
      "Document: file, quarantine, send, box, folder, program, false, software, local, capture, installation, save, zip, rtl, always, database, excluding, thanks, positive\n",
      " \n",
      "原文: [WFBS-SVC] Program falsely detected as malicious\n",
      "top topic: 9\n",
      "Document: detect, program, false, malicious\n",
      " \n"
     ]
    }
   ],
   "source": [
    "display_document_count=20\n",
    "display_keyword_count_per_topic=5\n",
    "\n",
    "doc_topic = model.transform(X)\n",
    "for i in range(display_document_count):\n",
    "    print('原文:', df.loc[i, 'Subject'])\n",
    "    doc=[]\n",
    "    #vocab_len = len(vocab)\n",
    "    #for j in range(vocab_len):\n",
    "    #    if X[i, j] != 0:\n",
    "    #        doc.append(vocab[j])\n",
    "    #print(' '.join(doc))\n",
    "    \n",
    "    print(\"top topic: {}\".format(doc_topic[i].argmax()))\n",
    "    top_n_words = X[i,:].argsort()[:display_keyword_count_per_topic:-1]\n",
    "    top_n_words_list = []\n",
    "    for k in top_n_words:\n",
    "        if X[i,k] > 0:\n",
    "            top_n_words_list.append(k)\n",
    "    print(\"Document: {}\".format(', '.join(np.array(vocab)[top_n_words_list])))\n",
    "    #print(\"Document: {}\".format(', '.join(np.array(vocab)[list(reversed(X[i,:].argsort()))[:display_keyword_count_per_topic:-1]])))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lighten the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step will delete some matrices inside the model.\n",
    "model.purge_extra_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "with open('guidedlda_model.pickle', 'wb') as file_handle:\n",
    "    pickle.dump(model, file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
