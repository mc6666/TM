{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Topics with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_data_list = ['[WFBS-SVC] Installation on Mac Is Not Full Featured',\n",
    "                  '[WFBS - A] Smart Scan is not updating',\n",
    "                  '[WFBS - SVC] Installation issue',\n",
    "                  '[MALWARE][WFBS SVC] Wallet Ransomware',\n",
    "                  '[MALWARE][WFBS S 9.5]Possible Ransomware detection on clients machine',\n",
    "                 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[WFBS-SVC] Installation on Mac Is Not Full Fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[WFBS - A] Smart Scan is not updating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[WFBS - SVC] Installation issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[MALWARE][WFBS SVC] Wallet Ransomware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MALWARE][WFBS S 9.5]Possible Ransomware detec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject\n",
       "0  [WFBS-SVC] Installation on Mac Is Not Full Fea...\n",
       "1              [WFBS - A] Smart Scan is not updating\n",
       "2                    [WFBS - SVC] Installation issue\n",
       "3              [MALWARE][WFBS SVC] Wallet Ransomware\n",
       "4  [MALWARE][WFBS S 9.5]Possible Ransomware detec..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(test_data_list, columns=['Subject'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import krovetz\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "break_words=list('\\n\\t')\n",
    "prefix_char_remove=list('.*,-')\n",
    "\n",
    "stemmer = krovetz.PyKrovetzStemmer()\n",
    "#stemmer = SnowballStemmer(\"english\")\n",
    "#stemmer.stem('working')\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "word_freqs = dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以 regular expression 去除 email、數字、URL、Phone No、序號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_pattern_list=[]\n",
    "re_pattern_list.append('[a-zA-Z0-9_]+\\.(com|org|net)[a-zA-Z0-9_\\.]*$') #email\n",
    "re_pattern_list.append('[0-9_]+') # all digits\n",
    "re_pattern_list.append(\"\\/\\/[\\w.-]+(?:\\.[\\w\\.-]+)+[\\w\\-\\._~:/?#[\\]@!\\$&'\\(\\)\\*\\+,;=.]+$\") # url\n",
    "re_pattern_list.append('[0-9\\-]+') # phone no\n",
    "re_pattern_list.append('[\\u0800-\\u4e00]+') # 日文\n",
    "re_pattern_list.append('(trend|trend\\smicro)') # trend micro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字詞清理函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean words\n",
    "def clean_word(word_0):\n",
    "    word_list=[]\n",
    "    for word in word_0.split('/'):\n",
    "\n",
    "        if len(word) <= 0 :\n",
    "            continue\n",
    "\n",
    "        # remove 網路芳鄰路徑\n",
    "        remote_path=word.split('\\\\')\n",
    "        if len(remote_path) > 0 and len(remote_path[-1])>0:\n",
    "            word=remote_path[-1]\n",
    "        elif len(remote_path) > 1 and len(remote_path[-2])>0:\n",
    "            word=remote_path[-2]\n",
    "        elif len(remote_path) > 2 and len(remote_path[-3])>0:\n",
    "            word=remote_path[-3]\n",
    "\n",
    "        if word[0] == '\"' and word[-1] == '\"':\n",
    "            word=word[1:-1]\n",
    "        if word[0] == \"'\" and word[-1] == \"'\":\n",
    "            word=word[1:-1]\n",
    "        if len(word) <= 0 :\n",
    "            continue\n",
    "\n",
    "        # repeat 3 times to remove continous chars\n",
    "        if word[0] in prefix_char_remove:\n",
    "            word=word[1:]\n",
    "        if len(word) <= 0 :\n",
    "            continue\n",
    "        if word[0] in prefix_char_remove:\n",
    "            word=word[1:]\n",
    "        if len(word) <= 0 :\n",
    "            continue\n",
    "        if word[0] in prefix_char_remove:\n",
    "            word=word[1:]\n",
    "\n",
    "        # lemmatize\n",
    "        word=word.strip().lower()\n",
    "        word=stemmer.stem(word)\n",
    "        if word=='':\n",
    "            continue      \n",
    "            \n",
    "        # remove .exe and .com\n",
    "        if word.endswith('.exe') or word.endswith('.com'):\n",
    "            word=word[:-4]\n",
    "            \n",
    "        # 以 regular expression 去除 email、數字、URL、Phone No、序號\n",
    "        is_re_list = False\n",
    "        for pattern1 in re_pattern_list:\n",
    "            result=re.findall(pattern1, word)\n",
    "            if  len(result) > 0:\n",
    "                is_re_list = True\n",
    "                break\n",
    "        if is_re_list:\n",
    "            break\n",
    "            \n",
    "            \n",
    "        # remove the words with one or two characters only\n",
    "        if len(word) <= 2 :\n",
    "            continue\n",
    "        if not (word in stop_words):\n",
    "            word_list.append(word)\n",
    "               \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern1 = '\\[[a-zA-Z0-9_\\-\\s]+\\]' # service_list pattern, e.g. [WFBS-SVC]\n",
    "\n",
    "# 清理後的 email DESCRIPTION\n",
    "clean_corpus=[]\n",
    "original_corpus=[]\n",
    "for index, line in df.iterrows():\n",
    "    clean_line=\"\"\n",
    "    line = line['Subject']\n",
    "    \n",
    "    # remove service_list, e.g. [WFBS-SVC]\n",
    "    if type(line) == str and len(line) > 0: \n",
    "        service_list = re.findall(pattern1, line)\n",
    "        for service_item in service_list:\n",
    "            line = line.replace(service_item, ' ')          \n",
    "                                  \n",
    "    #print(line)\n",
    "    for break_word in break_words:\n",
    "        #print('-',len(break_word), break_word, '-')\n",
    "        if not type(line) == str or len(line) <= 0: \n",
    "            break\n",
    "        line = line.replace(break_word, ' ')\n",
    "    if not type(line) == str or len(line) <= 0: \n",
    "        #print(type(line))\n",
    "        continue\n",
    "    words = word_tokenize(line) #line.lower().split(' ')\n",
    "    for word_0 in words:        \n",
    "        word_list = clean_word(word_0)\n",
    "        for word in word_list:        \n",
    "            if word in word_freqs:\n",
    "                word_freqs[word] += 1\n",
    "            else:\n",
    "                word_freqs[word] = 1\n",
    "            clean_line+=' '+word\n",
    "    original_corpus.append(line)\n",
    "    clean_corpus.append(clean_line.strip())\n",
    "email_words=word_freqs.keys()            \n",
    "print(len(email_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "keyword_list_new=[]\n",
    "keyword_list = set(email_words)\n",
    "for item in keyword_list:\n",
    "    is_re_list = False\n",
    "    for pattern1 in re_pattern_list:\n",
    "        result=re.findall(pattern1, item)\n",
    "        if  len(result) > 0:\n",
    "            is_re_list = True\n",
    "            break\n",
    "    if is_re_list == False:\n",
    "        keyword_list_new.append(item)\n",
    "len(keyword_list_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load List of Microsoft software and Ubuntu Glossaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"keyword_list.pickle\", 'rb') as f:\n",
    "    keyword_list_TERM = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare corpus with n-gram keywords, n=2~4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_special_term(words, start_index, n_gram):\n",
    "    merge_word=[]\n",
    "    for i in range(n_gram):\n",
    "        merge_word.append(words[start_index+i])\n",
    "    if ' '.join(merge_word) in keyword_list_TERM:\n",
    "        #print('converted_word=', keyword_list_TERM[' '.join(merge_word)]['converted_word'])\n",
    "        return keyword_list_TERM[' '.join(merge_word)]['converted_word']\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def compare_corpus_with_keyword(clean_corpus):\n",
    "    new_clean_corpus=[]\n",
    "    merge_word_list=[] # n-gram keywords\n",
    "    hit_row_index=[] # keep for predict test\n",
    "    for no, line in enumerate(clean_corpus):\n",
    "        words = line.split(' ')\n",
    "        #words = word_tokenize(line) \n",
    "        words_len = len(words)\n",
    "        # n-gram\n",
    "        new_line=''\n",
    "        for i in range(4, 1, -1):\n",
    "            for j in range(words_len-i+1):\n",
    "                merge_word = check_special_term(words, j, i)\n",
    "                if merge_word == '':\n",
    "                    new_line+=' '+words[j]\n",
    "                    if j==words_len-i:\n",
    "                        for k in range(j+1, words_len):\n",
    "                            new_line+=' '+words[k]\n",
    "                    continue\n",
    "                else:\n",
    "                    merge_word_list.append(merge_word)\n",
    "                    hit_row_index.append(no)\n",
    "                    new_line+=' '+merge_word\n",
    "                    j+=i\n",
    "                    continue\n",
    "\n",
    "            # line is too short, keep it as original\n",
    "            if words_len-i<0:\n",
    "                new_line=line    \n",
    "            new_line = new_line.strip()\n",
    "            words = new_line.split(' ')\n",
    "            words_len = len(words)\n",
    "            if i>2:\n",
    "                new_line=''\n",
    "        new_clean_corpus.append(new_line)\n",
    "        #if new_line != line:\n",
    "        #    print('org_line='+line+'\\n')\n",
    "        #    print('new_line='+new_line+'\\n\\n')\n",
    "    return new_clean_corpus, merge_word_list, hit_row_index\n",
    "\n",
    "new_clean_corpus, merge_word_list, hit_row_index = compare_corpus_with_keyword(clean_corpus)\n",
    "hit_row_index=list(set(hit_row_index))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer = CountVectorizer(decode_error=\"replace\",vocabulary=pickle.load(open(\"feature.pkl\", \"rb\")))\n",
    "\n",
    "BOW_vector = vectorizer.fit_transform(new_clean_corpus)\n",
    "X = BOW_vector.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = pickle.load(open(\"word2id.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import guidedlda\n",
    "\n",
    "with open('guidedlda_model.pickle', 'rb') as file_handle:\n",
    "    model = pickle.load(file_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: [WFBS-SVC] Installation on Mac Is Not Full Featured\n",
      "top topic: 4\n",
      "Document: mac, full, feature, installation\n",
      " \n",
      "原文: [WFBS - A] Smart Scan is not updating\n",
      "top topic: 6\n",
      "Document: smart, scan, update\n",
      " \n",
      "原文: [WFBS - SVC] Installation issue\n",
      "top topic: 4\n",
      "Document: issue, installation\n",
      " \n",
      "原文: [MALWARE][WFBS SVC] Wallet Ransomware\n",
      "top topic: 9\n",
      "Document: ransomware, wallet\n",
      " \n",
      "原文: [MALWARE][WFBS S 9.5]Possible Ransomware detection on clients machine\n",
      "top topic: 9\n",
      "Document: machine, detection, wfb, possible, ransomware, client\n",
      " \n"
     ]
    }
   ],
   "source": [
    "display_keyword_count_per_topic=5\n",
    "\n",
    "doc_topic = model.transform(X)\n",
    "display_document_count=X.shape[0]\n",
    "for i in range(display_document_count):\n",
    "    print('原文:', df.loc[i, 'Subject'])\n",
    "    doc=[]\n",
    "    #vocab_len = len(vocab)\n",
    "    #for j in range(vocab_len):\n",
    "    #    if X[i, j] != 0:\n",
    "    #        doc.append(vocab[j])\n",
    "    #print(' '.join(doc))\n",
    "    \n",
    "    print(\"top topic: {}\".format(doc_topic[i].argmax()))\n",
    "    top_n_words = X[i,:].argsort()[:display_keyword_count_per_topic:-1]\n",
    "    top_n_words_list = []\n",
    "    for k in top_n_words:\n",
    "        if X[i,k] > 0:\n",
    "            top_n_words_list.append(k)\n",
    "    print(\"Document: {}\".format(', '.join(np.array(vocab)[top_n_words_list])))\n",
    "    #print(\"Document: {}\".format(', '.join(np.array(vocab)[list(reversed(X[i,:].argsort()))[:display_keyword_count_per_topic:-1]])))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
